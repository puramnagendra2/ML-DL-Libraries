{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Communal violence in Bhainsa, Telangana. \"Ston...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Telangana: Section 144 has been imposed in Bha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>New York City</td>\n",
       "      <td>Arsonist sets cars ablaze at dealership https:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Morgantown, WV</td>\n",
       "      <td>Arsonist sets cars ablaze at dealership https:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Lord Jesus, your love brings freedom and pard...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword        location  \\\n",
       "0   0  ablaze             NaN   \n",
       "1   1  ablaze             NaN   \n",
       "2   2  ablaze   New York City   \n",
       "3   3  ablaze  Morgantown, WV   \n",
       "4   4  ablaze             NaN   \n",
       "\n",
       "                                                text  target  \n",
       "0  Communal violence in Bhainsa, Telangana. \"Ston...       1  \n",
       "1  Telangana: Section 144 has been imposed in Bha...       1  \n",
       "2  Arsonist sets cars ablaze at dealership https:...       1  \n",
       "3  Arsonist sets cars ablaze at dealership https:...       1  \n",
       "4  \"Lord Jesus, your love brings freedom and pard...       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['id', 'keyword', 'location'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    9256\n",
       "1    2114\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting Value Counts\n",
    "\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Communal violence in Bhainsa, Telangana. \"Stones were pelted on Muslims' houses and some houses and vehicles were set ablaze…</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Telangana: Section 144 has been imposed in Bhainsa from January 13 to 15, after clash erupted between two groups on January 12. Po…</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arsonist sets cars ablaze at dealership https://t.co/gOQvyJbpVI</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arsonist sets cars ablaze at dealership https://t.co/0gL7NUCPlb https://t.co/u1CcBhOWh9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Lord Jesus, your love brings freedom and pardon. Fill me with your Holy Spirit and set my heart ablaze with your l… https://t.co/VlTznnPNi8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           text  \\\n",
       "0  Communal violence in Bhainsa, Telangana. \"Stones were pelted on Muslims' houses and some houses and vehicles were set ablaze…                  \n",
       "1  Telangana: Section 144 has been imposed in Bhainsa from January 13 to 15, after clash erupted between two groups on January 12. Po…            \n",
       "2  Arsonist sets cars ablaze at dealership https://t.co/gOQvyJbpVI                                                                                \n",
       "3  Arsonist sets cars ablaze at dealership https://t.co/0gL7NUCPlb https://t.co/u1CcBhOWh9                                                        \n",
       "4  \"Lord Jesus, your love brings freedom and pardon. Fill me with your Holy Spirit and set my heart ablaze with your l… https://t.co/VlTznnPNi8   \n",
       "\n",
       "   target  \n",
       "0  1       \n",
       "1  1       \n",
       "2  1       \n",
       "3  1       \n",
       "4  0       "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower Casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "method1 = df['text'].str.lower()\n",
    "method2 = df['text'].apply(str.lower)\n",
    "method3 = df['text'].apply(lambda x : x.lower())\n",
    "method4 = df['text'].map(str.lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Communal violence in Bhainsa Telangana Stones were pelted on Muslims houses and some houses and vehicles were set ablaze…           \n",
       "1    Telangana Section 144 has been imposed in Bhainsa from January 13 to 15 after clash erupted between two groups on January 12 Po…    \n",
       "2    Arsonist sets cars ablaze at dealership httpstcogOQvyJbpVI                                                                          \n",
       "3    Arsonist sets cars ablaze at dealership httpstco0gL7NUCPlb httpstcou1CcBhOWh9                                                       \n",
       "4    Lord Jesus your love brings freedom and pardon Fill me with your Holy Spirit and set my heart ablaze with your l… httpstcoVlTznnPNi8\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1\n",
    "punc1 = df['text'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "# Method 2\n",
    "import string\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "punc2 = df['text'].apply(lambda x: x.translate(translator))\n",
    "\n",
    "# Method 3\n",
    "def remove_punctuation(text):\n",
    "    return ''.join([char for char in text if char not in string.punctuation])\n",
    "punc3 = df['text'].apply(remove_punctuation)\n",
    "\n",
    "# Method 4\n",
    "import re\n",
    "def remove_punctuation_with_re(text):\n",
    "    return re.sub(r'^[\\w\\s]','', text)\n",
    "punc4 = df['text'].apply(remove_punctuation_with_re)\n",
    "\n",
    "# Method 5\n",
    "def remove_punctuation_with_filter(text):\n",
    "    return ''.join(filter(lambda x: x not in string.punctuation, text))\n",
    "punc5 = df['text'].apply(remove_punctuation_with_filter)\n",
    "punc5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Communal violence in Bhainsa, Telangana. \"Stones were pelted on Muslims' houses and some houses and vehicles were set ablaze…              \n",
       "1    Telangana: Section  has been imposed in Bhainsa from January  to , after clash erupted between two groups on January . Po…                 \n",
       "2    Arsonist sets cars ablaze at dealership https://t.co/gOQvyJbpVI                                                                            \n",
       "3    Arsonist sets cars ablaze at dealership https://t.co/gLNUCPlb https://t.co/uCcBhOWh                                                        \n",
       "4    \"Lord Jesus, your love brings freedom and pardon. Fill me with your Holy Spirit and set my heart ablaze with your l… https://t.co/VlTznnPNi\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1\n",
    "rem_num1 = df['text'].str.replace(r'\\d+', '', regex=True)\n",
    "\n",
    "# Method 2\n",
    "def remove_numbers_with_isdigits(text):\n",
    "    return ''.join([char for char in text if not char.isdigit()])\n",
    "rem_num2 = df['text'].apply(remove_numbers_with_isdigits)\n",
    "\n",
    "# Method 3\n",
    "import string\n",
    "translation_table = str.maketrans('', '', string.digits)\n",
    "rem_num3 = df['text'].apply(lambda x: x.translate(translation_table))\n",
    "\n",
    "# Method 4\n",
    "import re\n",
    "def remove_numbers_with_re(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "rem_num4 = df['text'].apply(remove_numbers_with_re)\n",
    "\n",
    "# Method 5\n",
    "def remove_numbers_with_filter(text):\n",
    "    return ''.join(filter(lambda x: not x.isdigit(), text))\n",
    "rem_num5 = df['text'].apply(remove_numbers_with_filter)\n",
    "\n",
    "rem_num5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Extra Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Communal violence in Bhainsa, Telangana. \"Stones were pelted on Muslims' houses and some houses and vehicles were set ablaze…               \n",
       "1    Telangana: Section 144 has been imposed in Bhainsa from January 13 to 15, after clash erupted between two groups on January 12. Po…         \n",
       "2    Arsonist sets cars ablaze at dealership https://t.co/gOQvyJbpVI                                                                             \n",
       "3    Arsonist sets cars ablaze at dealership https://t.co/0gL7NUCPlb https://t.co/u1CcBhOWh9                                                     \n",
       "4    \"Lord Jesus, your love brings freedom and pardon. Fill me with your Holy Spirit and set my heart ablaze with your l… https://t.co/VlTznnPNi8\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Communal violence in Bhainsa, Telangana. \"Stones were pelted on Muslims' houses and some houses and vehicles were set ablaze…               \n",
       "1    Telangana: Section 144 has been imposed in Bhainsa from January 13 to 15, after clash erupted between two groups on January 12. Po…         \n",
       "2    Arsonist sets cars ablaze at dealership https://t.co/gOQvyJbpVI                                                                             \n",
       "3    Arsonist sets cars ablaze at dealership https://t.co/0gL7NUCPlb https://t.co/u1CcBhOWh9                                                     \n",
       "4    \"Lord Jesus, your love brings freedom and pardon. Fill me with your Holy Spirit and set my heart ablaze with your l… https://t.co/VlTznnPNi8\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1\n",
    "rem_space1 = df['text'].str.strip().str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Method 2\n",
    "def remove_extra_spaces(text):\n",
    "    return ' '.join(text.split())\n",
    "rem_space2 = df['text'].apply(remove_extra_spaces)\n",
    "\n",
    "# Method 3\n",
    "rem_space3 = df['text'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Method 4\n",
    "import re\n",
    "def remove_extra_spaces_with_re(text):\n",
    "    return re.sub(r'\\s+', ' ', text.strip())\n",
    "\n",
    "rem_space4 = df['text'].apply(remove_extra_spaces_with_re)\n",
    "\n",
    "# Method 5\n",
    "def remove_extra_spaces_filter(text):\n",
    "    return ' '.join(filter(lambda x: x.strip(), text.split()))\n",
    "rem_space5 = df['text'].apply(remove_extra_spaces_filter)\n",
    "\n",
    "rem_space5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing Repeated Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Communal violence in Bhainsa, Telangana. \"Stones were pelted on Muslims' houses and some houses and vehicles were set ablaze…               \n",
       "1    Telangana: Section 144 has been imposed in Bhainsa from January 13 to 15, after clash erupted between two groups on January 12. Po…         \n",
       "2    Arsonist sets cars ablaze at dealership https://t.co/gOQvyJbpVI                                                                             \n",
       "3    Arsonist sets cars ablaze at dealership https://t.co/0gL7NUCPlb https://t.co/u1CcBhOWh9                                                     \n",
       "4    \"Lord Jesus, your love brings freedom and pardon. Fill me with your Holy Spirit and set my heart ablaze with your l… https://t.co/VlTznnPNi8\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Communal violence in Bhainsa, Telangana. \"Stones were pelted on Muslims' houses and some houses and vehicles were set ablaze…              \n",
       "1    Telangana: Section 144 has been imposed in Bhainsa from January 13 to 15, after clash erupted between two groups on January 12. Po…        \n",
       "2    Arsonist sets cars ablaze at dealership https:/t.co/gOQvyJbpVI                                                                             \n",
       "3    Arsonist sets cars ablaze at dealership https:/t.co/0gL7NUCPlb https:/t.co/u1CcBhOWh9                                                      \n",
       "4    \"Lord Jesus, your love brings freedom and pardon. Fill me with your Holy Spirit and set my heart ablaze with your l… https:/t.co/VlTznnPNi8\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1\n",
    "punc_rep1 = df['text'].str.replace(r'([!?./\\@])\\1+', r'\\1', regex=True)\n",
    "\n",
    "# Method 2\n",
    "import re\n",
    "def replace_repeated_puncs(text):\n",
    "    return re.sub(r'([!?/\\.])\\1+', r'\\1', text)\n",
    "punc_rep2 = df['text'].apply(replace_repeated_puncs)\n",
    "\n",
    "# Method 3\n",
    "def remove_repeated_punc_lists(text):\n",
    "    result = []\n",
    "    for char in text:\n",
    "        if result and char ==  result[-1] and char in \"!?/\\.\":\n",
    "            continue\n",
    "        result.append(char)\n",
    "    return ''.join(result)\n",
    "punc_rep3 = df['text'].apply(remove_repeated_punc_lists)\n",
    "\n",
    "# Method 4\n",
    "def replace_repeated_puncs_translate(text):\n",
    "    punctuations = r'!/\\?.@'\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p * 2, p)\n",
    "    return text\n",
    "punc_rep4 = df['text'].apply(replace_repeated_puncs_translate)\n",
    "\n",
    "punc_rep4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello there! 😀 How are you? 🤔</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love programming! 💻✨</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Let's remove emojis! 🎉🎈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No emojis here.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text\n",
       "0  Hello there! 😀 How are you? 🤔\n",
       "1  I love programming! 💻✨       \n",
       "2  Let's remove emojis! 🎉🎈      \n",
       "3  No emojis here.              "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset containing emojis\n",
    "df_emoji = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"Hello there! 😀 How are you? 🤔\",\n",
    "        \"I love programming! 💻✨\",\n",
    "        \"Let's remove emojis! 🎉🎈\",\n",
    "        \"No emojis here.\"\n",
    "    ]\n",
    "})\n",
    "df_emoji.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clean-text in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.6.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: demoji in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clean-text) (1.7.0)\n",
      "Requirement already satisfied: ftfy<7.0,>=6.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from clean-text) (6.3.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.13)\n"
     ]
    }
   ],
   "source": [
    "# pip install clean-text\n",
    "%pip install clean-text demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11664\\2078455835.py:20: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    Hello there!  How are you? \n",
       "1    I love programming!        \n",
       "2    Let's remove emojis!       \n",
       "3    No emojis here.            \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1\n",
    "from cleantext import clean\n",
    "emoji_rem1 = df_emoji['text'].apply(lambda x: clean(x, no_emoji=True))\n",
    "\n",
    "# Method 2\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                 \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                 \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                 \"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                                 \"\\U0001F900-\\U0001F9FF\"  # supplemental symbols and pictographs\n",
    "                                 \"\\U00002702-\\U000027B0\"  # dingbats\n",
    "                                 \"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
    "                                 \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "emoji_rem2 = df_emoji['text'].apply(remove_emojis)\n",
    "\n",
    "# Method 3\n",
    "import demoji\n",
    "demoji.download_codes()\n",
    "emoji_rem3 = df_emoji['text'].apply(lambda x: demoji.replace(x, \"\"))\n",
    "\n",
    "# Method 4\n",
    "emoji_rem4 = df_emoji['text'].apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "\n",
    "emoji_rem4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello there! :) How are you? :D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love programming! &lt;3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Let's remove emoticons! :P :o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No emoticons here.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              text\n",
       "0  Hello there! :) How are you? :D\n",
       "1  I love programming! <3         \n",
       "2  Let's remove emoticons! :P :o  \n",
       "3  No emoticons here.             "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset for Emoticons\n",
    "df_emoticons = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"Hello there! :) How are you? :D\",\n",
    "        \"I love programming! <3\",\n",
    "        \"Let's remove emoticons! :P :o\",\n",
    "        \"No emoticons here.\"\n",
    "    ]\n",
    "})\n",
    "df_emoticons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Hello there!  How are you? \n",
       "1    I love programming! <3     \n",
       "2    Let's remove emoticons!    \n",
       "3    No emoticons here.         \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1\n",
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(r'[:;=][)DdpP\\(\\[<3]')\n",
    "    return emoticon_pattern.sub('', text)\n",
    "emoticon_rem1 = df_emoticons['text'].apply(remove_emoticons)\n",
    "\n",
    "# Method 2\n",
    "emoticon_rem2 = df_emoticons['text'].str.replace(r'[:;=][)DdoOpP\\(\\[<]', '', regex=True)\n",
    "\n",
    "# Method 3\n",
    "emoticons = [':)', ':(', ':D', ':P', '<3', ':o']\n",
    "\n",
    "def remove_custom_emoticons(text):\n",
    "    for emoticon in emoticons:\n",
    "        text = text.replace(emoticon, '')\n",
    "    return text\n",
    "emoticon_rem3 = df_emoticons['text'].apply(remove_custom_emoticons)\n",
    "\n",
    "emoticon_rem2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'll be there within 5 min.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She'd like to know how I'd done that!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's awesome to meet new friends.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We've been waiting for this day for so long.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           text\n",
       "0  I'll be there within 5 min.                 \n",
       "1  She'd like to know how I'd done that!       \n",
       "2  It's awesome to meet new friends.           \n",
       "3  We've been waiting for this day for so long."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset for contractions\n",
    "df_con = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"I'll be there within 5 min.\",\n",
    "        \"She'd like to know how I'd done that!\",\n",
    "        \"It's awesome to meet new friends.\",\n",
    "        \"We've been waiting for this day for so long.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "df_con.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I will be there within 5 min.                 \n",
       "1    She would like to know how I would done that! \n",
       "2    It is awesome to meet new friends.            \n",
       "3    We have been waiting for this day for so long.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1\n",
    "import contractions\n",
    "def expand_contractions(text):\n",
    "    return ' '.join([contractions.fix(word) for word in text.split()])\n",
    "con1 = df_con['text'].apply(expand_contractions)\n",
    "\n",
    "# Method 2\n",
    "contraction_mapping = {\n",
    "    \"I'll\": \"I will\",\n",
    "    \"She'd\": \"She would\",\n",
    "    \"It's\": \"It is\",\n",
    "    \"We've\": \"We have\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"can't\": \"cannot\"\n",
    "}\n",
    "def expand_contractions_regex(text):\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(contraction_mapping.keys()) + r')\\b')\n",
    "    return pattern.sub(lambda x: contraction_mapping[x.group(0)], text)\n",
    "con2 = df_con['text'].apply(expand_contractions_regex)\n",
    "\n",
    "# Method 3\n",
    "contraction_mapping = {\n",
    "    \"I'll\": \"I will\",\n",
    "    \"She'd\": \"She would\",\n",
    "    \"It's\": \"It is\",\n",
    "    \"We've\": \"We have\",\n",
    "    \"I'd\": \"I would\",\n",
    "}\n",
    "\n",
    "# Function to expand contractions using manual mapping\n",
    "def manual_expand_contractions(text):\n",
    "    for contraction, expansion in contraction_mapping.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    return text\n",
    "con3 = df_con['text'].apply(manual_expand_contractions)\n",
    "\n",
    "con3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding and Removing Html Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;h1&gt;Hello World!&lt;/h1&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;p&gt;This is a &lt;strong&gt;test&lt;/strong&gt; string.&lt;/p&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;div&gt;Another &lt;em&gt;example&lt;/em&gt; with &lt;a href='#'&gt;links&lt;/a&gt;.&lt;/div&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No HTML tags here.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              text\n",
       "0  <h1>Hello World!</h1>                                          \n",
       "1  <p>This is a <strong>test</strong> string.</p>                 \n",
       "2  <div>Another <em>example</em> with <a href='#'>links</a>.</div>\n",
       "3  No HTML tags here.                                             "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset\n",
    "df_html = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"<h1>Hello World!</h1>\",\n",
    "        \"<p>This is a <strong>test</strong> string.</p>\",\n",
    "        \"<div>Another <em>example</em> with <a href='#'>links</a>.</div>\",\n",
    "        \"No HTML tags here.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "df_html.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "def remove_html_tags(text):\n",
    "    return re.sub(r'<.*?>', '', text)\n",
    "html_rem1 = df_html['text'].apply(remove_html_tags)\n",
    "\n",
    "# Method 2\n",
    "from bs4 import BeautifulSoup\n",
    "def remove_html_tags_bs(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "html_rem2 = df_html['text'].apply(remove_html_tags_bs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding and Removing URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Check out this link: https://www.example.com and let me know what you think.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Visit http://example.org for more information.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No links here, just plain text.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Another link: www.test.com is also available.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           Text\n",
       "0  Check out this link: https://www.example.com and let me know what you think.\n",
       "1  Visit http://example.org for more information.                              \n",
       "2  No links here, just plain text.                                             \n",
       "3  Another link: www.test.com is also available.                               "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset\n",
    "url_df = pd.DataFrame({\n",
    "    'Text': [\n",
    "        \"Check out this link: https://www.example.com and let me know what you think.\",\n",
    "        \"Visit http://example.org for more information.\",\n",
    "        \"No links here, just plain text.\",\n",
    "        \"Another link: www.test.com is also available.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "url_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Check out this and let me know what you think.\n",
       "1    Visit for more information.                   \n",
       "2    No links here, just plain text.               \n",
       "3    Another www.test.com is also available.       \n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1\n",
    "url_rem1 = url_df['Text'].str.replace(r'http[s]?://\\S+|www\\.\\S+', '', regex=True)\n",
    "\n",
    "# Method 2\n",
    "def url_remove(text):\n",
    "    return re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
    "url_rem2 = url_df['Text'].apply(url_remove)\n",
    "\n",
    "# Method 3\n",
    "url_rem3 = url_df['Text'].apply(lambda x: clean(x, no_urls=True, lower=False, replace_with_url='', ))\n",
    "\n",
    "# Method 4\n",
    "from urllib.parse import urlparse\n",
    "def url_remove_parser(text):\n",
    "    words = text.split()\n",
    "    return ' '.join(word for word in words if not urlparse(word).scheme)\n",
    "\n",
    "url_rem4 = url_df['Text'].apply(url_remove_parser)\n",
    "url_rem4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Contact us at support@example.com for assistance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My email is john.doe@gmail.com and I need help.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No emails here, just plain text.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reach out to info@company.org.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  Contact us at support@example.com for assistance.\n",
       "1  My email is john.doe@gmail.com and I need help.  \n",
       "2  No emails here, just plain text.                 \n",
       "3  Reach out to info@company.org.                   "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset\n",
    "email_df = pd.DataFrame({\n",
    "    'Text': [\n",
    "        \"Contact us at support@example.com for assistance.\",\n",
    "        \"My email is john.doe@gmail.com and I need help.\",\n",
    "        \"No emails here, just plain text.\",\n",
    "        \"Reach out to info@company.org.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "email_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Contact us at for assistance.   \n",
       "1    My email is and I need help.    \n",
       "2    No emails here, just plain text.\n",
       "3    Reach out to info@company.org.  \n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1\n",
    "email_rem1 = email_df['Text'].str.replace(r'\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b', '', regex=True)\n",
    "\n",
    "# Method 2\n",
    "def remove_emails(text):\n",
    "    return re.sub(r'\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b', '', text)\n",
    "email_rem2 = email_df['Text'].apply(remove_emails)\n",
    "\n",
    "# Method 3 (Makes all small case)\n",
    "email_rem3 = email_df['Text'].apply(lambda x: clean(x, lower=False, no_emails=True, replace_with_email=\"\"))\n",
    "\n",
    "# Method 4 \n",
    "def is_email(word):\n",
    "    return re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$', word) is not None\n",
    "\n",
    "def remove_emails_custom(text):\n",
    "    words = text.split()\n",
    "    return ' '.join(word for word in words if not is_email(word))\n",
    "email_rem4 = email_df['Text'].apply(remove_emails_custom)\n",
    "\n",
    "email_rem4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting URL & Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Contact us at support@example.com for assistance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My email is john.doe@gmail.com and I need help.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No emails here, just plain text.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reach out to info@company.org.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  Contact us at support@example.com for assistance.\n",
       "1  My email is john.doe@gmail.com and I need help.  \n",
       "2  No emails here, just plain text.                 \n",
       "3  Reach out to info@company.org.                   "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [support@example.com]\n",
       "1    [john.doe@gmail.com] \n",
       "2    []                   \n",
       "3    [info@company.org]   \n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using str functions\n",
    "get_email1 = email_df['Text'].str.extract(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})')\n",
    "\n",
    "# Using regex library\n",
    "import re\n",
    "def extract_emails(text):\n",
    "    return re.findall(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})', text)\n",
    "get_email2 = email_df['Text'].apply(extract_emails)\n",
    "\n",
    "get_email2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Check out this link: https://www.example.com and let me know what you think.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Visit http://example.org for more information.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No links here, just plain text.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Another link: www.test.com is also available.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           Text\n",
       "0  Check out this link: https://www.example.com and let me know what you think.\n",
       "1  Visit http://example.org for more information.                              \n",
       "2  No links here, just plain text.                                             \n",
       "3  Another link: www.test.com is also available.                               "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.example.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://example.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>www.test.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0\n",
       "0  https://www.example.com\n",
       "1  http://example.org     \n",
       "2  NaN                    \n",
       "3  www.test.com           "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using str function\n",
    "expression = r'(https?://[^\\s]+|www\\.[^\\s]+)'\n",
    "get_url1 = url_df['Text'].str.extract(pat=expression, expand=True)\n",
    "\n",
    "# Using regex\n",
    "def extract_urls(text):\n",
    "    return re.findall(r'(https?://[^\\s]+|www\\.[^\\s]+)', text)\n",
    "get_url2 = url_df['Text'].apply(extract_urls)\n",
    "\n",
    "get_url1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing and Spell Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.18.0.post0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pyspellchecker in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.8->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk>=3.8->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "%pip install textblob pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I havv a dreem.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is an exmple of spell chekcing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python is a gr8 programming language.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lets go to the park tommorow.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Text\n",
       "0  I havv a dreem.                      \n",
       "1  This is an exmple of spell chekcing. \n",
       "2  Python is a gr8 programming language.\n",
       "3  Lets go to the park tommorow.        "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "spell_df = pd.DataFrame({\n",
    "    'Text': [\n",
    "        \"I havv a dreem.\",\n",
    "        \"This is an exmple of spell chekcing.\",\n",
    "        \"Python is a gr8 programming language.\",\n",
    "        \"Lets go to the park tommorow.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "spell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: manual_spellchecker in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from manual_spellchecker) (4.67.1)\n",
      "Requirement already satisfied: pyenchant in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from manual_spellchecker) (3.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from manual_spellchecker) (1.26.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->manual_spellchecker) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install autocorrect manual_spellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I have a dream.                      \n",
       "1    His is an example of spell checking. \n",
       "2    Python is a grm programming language.\n",
       "3    Gets go to the park tomorrow.        \n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using TextBlob\n",
    "def correct_spelling(text):\n",
    "    return str(TextBlob(text).correct())\n",
    "spell1 = spell_df['Text'].apply(correct_spelling)\n",
    "\n",
    "# Using autocorrect (Gives good results)\n",
    "import itertools\n",
    "from autocorrect import Speller\n",
    "def standardize_text(text):\n",
    "    spell = Speller(lang='en')\n",
    "    return spell(text)\n",
    "spell2 = spell_df['Text'].apply(standardize_text)\n",
    "\n",
    "spell1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization is process of splitting a phrase, sentence, paragraph or text in a document into smaller units called 'TOKENS'.\n",
    "* Tokens are building blocks of Natural Language.\n",
    "* Tokens are used to construct Vocabulary [ set of unique tokens].\n",
    "* Types of Tokenization:\n",
    "    * Sentence Tokenization\n",
    "    * Word Tokenization\n",
    "    * Sub-words Tokenization\n",
    "    * Character Tokenization\n",
    "    * Regular Expression Tokenization (for custom tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is the first sentence. This is the second sentence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog. It is a common saying.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence 1. Sentence 2! Sentence 3?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a longer sentence with multiple clauses and punctuation marks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A single sentence without any punctuation.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     text\n",
       "0  This is the first sentence. This is the second sentence.              \n",
       "1  The quick brown fox jumps over the lazy dog. It is a common saying.   \n",
       "2  Sentence 1. Sentence 2! Sentence 3?                                   \n",
       "3  This is a longer sentence with multiple clauses and punctuation marks.\n",
       "4  A single sentence without any punctuation.                            "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'text': [\n",
    "        \"This is the first sentence. This is the second sentence.\",\n",
    "        \"The quick brown fox jumps over the lazy dog. It is a common saying.\",\n",
    "        \"Sentence 1. Sentence 2! Sentence 3?\",\n",
    "        \"This is a longer sentence with multiple clauses and punctuation marks.\",\n",
    "        \"A single sentence without any punctuation.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "sam = pd.DataFrame(data)\n",
    "sam.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python str function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [This is the first sentence,  This is the second sentence, ]             \n",
       "1    [The quick brown fox jumps over the lazy dog,  It is a common saying, ]  \n",
       "2    [Sentence 1,  Sentence 2! Sentence 3?]                                   \n",
       "3    [This is a longer sentence with multiple clauses and punctuation marks, ]\n",
       "4    [A single sentence without any punctuation, ]                            \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_token1 = sam['text'].str.split('.')\n",
    "sent_token1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Regex Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [This is the first sentence., This is the second sentence.]             \n",
       "1    [The quick brown fox jumps over the lazy dog., It is a common saying.]  \n",
       "2    [Sentence 1., Sentence 2!, Sentence 3?]                                 \n",
       "3    [This is a longer sentence with multiple clauses and punctuation marks.]\n",
       "4    [A single sentence without any punctuation.]                            \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sent_token2 = sam['text'].apply(lambda x: re.split(r'(?<=[.!?]) +', x))\n",
    "sent_token2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [This is the first sentence., This is the second sentence.]             \n",
       "1    [The quick brown fox jumps over the lazy dog., It is a common saying.]  \n",
       "2    [Sentence 1., Sentence 2!, Sentence 3?]                                 \n",
       "3    [This is a longer sentence with multiple clauses and punctuation marks.]\n",
       "4    [A single sentence without any punctuation.]                            \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using NLTK\n",
    "from nltk.tokenize import sent_tokenize\n",
    "def sentence_tokenize(text):\n",
    "    return sent_tokenize(text, language='english')\n",
    "sent_token3 = sam['text'].apply(sentence_tokenize)\n",
    "sent_token3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (8.2.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.13.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.10.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [This is the first sentence., This is the second sentence.]             \n",
       "1    [The quick brown fox jumps over the lazy dog., It is a common saying.]  \n",
       "2    [Sentence 1., Sentence 2! Sentence 3?]                                  \n",
       "3    [This is a longer sentence with multiple clauses and punctuation marks.]\n",
       "4    [A single sentence without any punctuation.]                            \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def sent_tokenize_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    return sentences\n",
    "\n",
    "#sent_token4 = sam['text'].apply(sent_tokenize_spacy)\n",
    "# or\n",
    "sent_token4 = sam['text'].apply(lambda x:[sent.text for sent in nlp(x).sents])\n",
    "\n",
    "sent_token4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [This, is, the, first, sentence., This, is, the, second, sentence.]               \n",
       "1    [The, quick, brown, fox, jumps, over, the, lazy, dog., It, is, a, common, saying.]\n",
       "2    [Sentence, 1., Sentence, 2!, Sentence, 3?]                                        \n",
       "3    [This, is, a, longer, sentence, with, multiple, clauses, and, punctuation, marks.]\n",
       "4    [A, single, sentence, without, any, punctuation.]                                 \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize1 = sam['text'].str.split(\" \")\n",
    "word_tokenize1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Regex Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [This, is, the, first, sentence, This, is, the, second, sentence]                \n",
       "1    [The, quick, brown, fox, jumps, over, the, lazy, dog, It, is, a, common, saying] \n",
       "2    [Sentence, 1, Sentence, 2, Sentence, 3]                                          \n",
       "3    [This, is, a, longer, sentence, with, multiple, clauses, and, punctuation, marks]\n",
       "4    [A, single, sentence, without, any, punctuation]                                 \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "word_tokenize2 = sam['text'].apply(lambda x: re.findall(r'\\b\\w+\\b', x))\n",
    "word_tokenize2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [This, is, the, first, sentence, ., This, is, the, second, sentence, .]               \n",
       "1    [The, quick, brown, fox, jumps, over, the, lazy, dog, ., It, is, a, common, saying, .]\n",
       "2    [Sentence, 1, ., Sentence, 2, !, Sentence, 3, ?]                                      \n",
       "3    [This, is, a, longer, sentence, with, multiple, clauses, and, punctuation, marks, .]  \n",
       "4    [A, single, sentence, without, any, punctuation, .]                                   \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize3 = sam['text'].apply(word_tokenize)\n",
    "word_tokenize3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [This, is, the, first, sentence, ., This, is, the, second, sentence, .]               \n",
       "1    [The, quick, brown, fox, jumps, over, the, lazy, dog, ., It, is, a, common, saying, .]\n",
       "2    [Sentence, 1, ., Sentence, 2, !, Sentence, 3, ?]                                      \n",
       "3    [This, is, a, longer, sentence, with, multiple, clauses, and, punctuation, marks, .]  \n",
       "4    [A, single, sentence, without, any, punctuation, .]                                   \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize4 = sam['text'].apply(lambda x: [token.text for token in nlp(x)])\n",
    "word_tokenize4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [This, is, the, first, sentence, This, is, the, second, sentence]                \n",
       "1    [The, quick, brown, fox, jumps, over, the, lazy, dog, It, is, a, common, saying] \n",
       "2    [Sentence, 1, Sentence, 2, Sentence, 3]                                          \n",
       "3    [This, is, a, longer, sentence, with, multiple, clauses, and, punctuation, marks]\n",
       "4    [A, single, sentence, without, any, punctuation]                                 \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "word_tokenize5 = sam['text'].apply(lambda x: TextBlob(x).words)\n",
    "word_tokenize5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [this, is, the, first, sentence, this, is, the, second, sentence]             \n",
       "1    [the, quick, brown, fox, jumps, over, the, lazy, dog, it, is, common, saying] \n",
       "2    [sentence, sentence, sentence]                                                \n",
       "3    [this, is, longer, sentence, with, multiple, clauses, and, punctuation, marks]\n",
       "4    [single, sentence, without, any, punctuation]                                 \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "word_tokenize4 = sam['text'].apply(simple_preprocess)\n",
    "word_tokenize4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [this, is, the, first, sentence, this, is, the, second, sentence]                \n",
       "1    [the, quick, brown, fox, jumps, over, the, lazy, dog, it, is, a, common, saying] \n",
       "2    [sentence, 1, sentence, 2, sentence, 3]                                          \n",
       "3    [this, is, a, longer, sentence, with, multiple, clauses, and, punctuation, marks]\n",
       "4    [a, single, sentence, without, any, punctuation]                                 \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "word_tokenize5 = sam['text'].apply(text_to_word_sequence)\n",
    "word_tokenize5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    This is the first sentence. This is the second sentence.              \n",
       "1    The quick brown fox jumps over the lazy dog. It is a common saying.   \n",
       "2    Sentence 1. Sentence 2! Sentence 3?                                   \n",
       "3    This is a longer sentence with multiple clauses and punctuation marks.\n",
       "4    A single sentence without any punctuation.                            \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.46.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: filelock in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.20.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [this, is, the, first, sentence, ., this, is, the, second, sentence, .]                     \n",
       "1    [the, quick, brown, fox, jumps, over, the, lazy, dog, ., it, is, a, common, saying, .]      \n",
       "2    [sentence, 1, ., sentence, 2, !, sentence, 3, ?]                                            \n",
       "3    [this, is, a, longer, sentence, with, multiple, clauses, and, pun, ##ct, ##uation, marks, .]\n",
       "4    [a, single, sentence, without, any, pun, ##ct, ##uation, .]                                 \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "auto_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_subword_tokenize1 = sam['text'].apply(lambda x: bert_tokenizer.tokenize(x))\n",
    "auto_subword_tokenize1 = sam['text'].apply(lambda x: auto_tokenizer.tokenize(x))\n",
    "# bert_subword_tokenize1.head()\n",
    "print(\"===================\")\n",
    "auto_subword_tokenize1.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
